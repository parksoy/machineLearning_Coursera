%%%%% Initializationclear ; close all; clcdata = load('ex2data1.txt');X = data(:, [1, 2]); y = data(:, 3);plotData(X, y);% ============ Part 2: Compute Cost and Gradient ============%  In this part of the exercise, you will implement the cost and gradient%  for logistic regression. You neeed to complete the code in %  costFunction.m%  Setup the data matrix appropriately, and add ones for the intercept term[m, n] = size(X);% Add intercept term to x and X_testX = [ones(m, 1) X];% Initialize fitting parametersinitial_theta = zeros(n + 1, 1);thetaX=X*initial_theta;h=sigmoid(thetaX);costJ=1/m*sum(-y.*log(h)-(1-y).*log(1-h));grad=1/m*sum((h-y)*X);% Compute and display initial cost and gradient%[cost, grad] = costFunction(initial_theta, X, y);%fprintf('Cost at initial theta (zeros): %f\n', cost);%fprintf('Gradient at initial theta (zeros): \n');%fprintf(' %f \n', grad);